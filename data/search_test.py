# search_test.py
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle

# 1. ëª¨ë¸ ë¡œë“œ
print("ëª¨ë¸ ë° ë°ì´í„° ë¡œë”©...")
model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# 2. FAISS ì¸ë±ìŠ¤ ë¡œë“œ
index = faiss.read_index('washing_machine.index')

# 3. ì²­í¬ ë©”íƒ€ë°ì´í„° ë¡œë“œ
with open('chunks.pkl', 'rb') as f:
    chunks = pickle.load(f)

print(f"ë¡œë“œ ì™„ë£Œ! (ì´ {len(chunks)}ê°œ ì²­í¬)")

# 4. ê²€ìƒ‰ í•¨ìˆ˜
def search(query, top_k=3):
    # ì¿¼ë¦¬ ì„ë² ë”©
    query_embedding = model.encode([query])
    query_embedding = np.array(query_embedding).astype('float32')
    
    # ê²€ìƒ‰
    distances, indices = index.search(query_embedding, top_k)
    
    print(f"\n{'='*70}")
    print(f"ì§ˆë¬¸: {query}")
    print('='*70)
    
    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
        chunk = chunks[idx]
        print(f"\n[{i+1}ìœ„] ê±°ë¦¬: {dist:.2f} | í˜ì´ì§€: {chunk['page']} | ì œëª©: {chunk['title']}")
        print("-"*70)
        print(chunk['content'][:300] + "...")
        print()

# 5. í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    search("ê¸‰ìˆ˜ í˜¸ìŠ¤ ì—°ê²° ë°©ë²•")
    search("ì„¸íƒê¸° ì—ëŸ¬ ì½”ë“œ")
    search("ê±´ì¡° ì‹œ ì£¼ì˜ì‚¬í•­")
    search("ì„¸ì œ ë„£ëŠ” ê³³")
    
    # ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ
    print("\n" + "="*70)
    print("ì§ì ‘ ì§ˆë¬¸í•´ë³´ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ì…ë ¥)")
    print("="*70)
    
    while True:
        query = input("\nì§ˆë¬¸: ")
        if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        if query.strip():
            search(query)